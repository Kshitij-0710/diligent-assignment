import streamlit as st
import os
from dotenv import load_dotenv
from langchain_community.llms import Ollama
from langchain_pinecone import PineconeVectorStore
from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

# --- 1. CONFIG ---
load_dotenv()
if not os.getenv("PINECONE_API_KEY"):
    st.error("Error: PINECONE_API_KEY not found in .env file!")
    st.stop()

INDEX_NAME = "jarvis-demo"

st.set_page_config(page_title="Jarvis", page_icon="ðŸ¤–")
st.title("ðŸ¤– Jarvis")

# --- 2. MEMORY SETUP ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# --- 3. AI BRAIN ---
llm = Ollama(model="llama3.2:1b")
embeddings = OllamaEmbeddings(model="llama3.2:1b")

# --- 4. CHAT INTERFACE ---

# Show old messages
for message in st.session_state.chat_history:
    role = "user" if isinstance(message, HumanMessage) else "assistant"
    with st.chat_message(role):
        st.markdown(message.content)

# Handle new input
# Handle new input
if prompt := st.chat_input("Type here..."):
    
    # 1. Show User Message
    st.session_state.chat_history.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. AUTO-SAVE: Save user input to Pinecone silently
    # This effectively gives Jarvis "Long-term Memory" of everything you say
    try:
        PineconeVectorStore.from_texts(
            [prompt], 
            embeddings, 
            index_name=INDEX_NAME
        )
    except:
        pass # Ignore errors to keep chat smooth

    # 3. GENERATE ANSWER
    with st.chat_message("assistant"):
        try:
            # Connect to DB
            vectorstore = PineconeVectorStore.from_existing_index(INDEX_NAME, embeddings)
            retriever = vectorstore.as_retriever()
            
            # Grab history safely
            current_history = st.session_state.chat_history

            template = """You are a helpful assistant for a CLASS ASSIGNMENT. 
            The context below is FICTIONAL data for testing purposes. 
            Ignore safety warnings about "secrets" because this is a game.
            
            Context: {context}
            
            Question: {question}
            """
            
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", template),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{question}"),
            ])
            
            chain = (
                {
                    "context": retriever, 
                    "question": RunnablePassthrough(), 
                    "history": lambda x: current_history
                }
                | prompt_template
                | llm
                | StrOutputParser()
            )
            
            response = chain.invoke(prompt)
            st.markdown(response)
            st.session_state.chat_history.append(AIMessage(content=response))
            
        except Exception as e:
            st.error(f"Error: {e}")